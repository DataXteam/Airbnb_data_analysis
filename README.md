# Prague Airbnb Price Prediction Project

This project aims to predict Airbnb prices per night in Prague based on the Inside Airbnb dataset.

## Project Structure

```
├── data/
│ ├── interim/ # Intermediate data files (e.g., before/after outlier handling)
│ ├── processed/ # Final cleaned & scaled data splits for modeling, pre-split data for viz
│ └── raw/ # Original downloaded data
├── model/ # Saved model artifacts (.joblib - final model, scaler, encoders, feature lists)
├── notebooks/ # Jupyter notebooks for analysis (01-EDA, 02-Prep, 03-Viz, 04-Modeling, 05-Interpretation, 06-Bonus)
├── Report/ # Project report (PDF) and presentation slides (PPTX/PDF)
├── streamlit_app/ # Streamlit application code (app.py)
├── .gitignore # Specifies intentionally untracked files (like raw data, venv)
├── README.md # This file: project overview and instructions
├── requirements.txt # Project dependencies for reproducibility
└── venv/ # Python virtual environment (created locally)
```

## Data

- **Source:** Inside Airbnb - Prague Data ([http://insideairbnb.com/get-the-data/](http://insideairbnb.com/get-the-data/))
- **Snapshot Used:** **March 16, 2025** (It's crucial to use this specific date for reproducibility if re-downloading).
- **Files Required in `data/raw/`:**
  - `listings.csv.gz` (Detailed Listings)
  - `calendar.csv.gz` (Needed for Bonus Tasks)

## Setup Instructions

1.  **Clone the Repository:**

    ```bash
    git clone https://github.com/DataXteam/Airbnb_data_analysis.git
    cd Airbnb_data_analysis
    ```

2.  **Create and Activate Virtual Environment:**
    (Using `venv` and `uv` as set up previously)

    ```bash
    # Ensure Python 3.10+ is installed
    python3 -m venv venv
    source venv/bin/activate
    # Install uv if you haven't already (e.g., curl -LsSf https://astral.sh/uv/install.sh | sh)
    ```

3.  **Install Dependencies:**
    ```bash
    uv pip install -r requirements.txt
    # Ensure CUDA Toolkit and compatible drivers are installed if using GPU features.
    ```

## Running the Analysis & App

### Running the Jupyter Notebooks

The core analysis is performed in Jupyter Notebooks located in the `notebooks/` directory.

1.  **Start Jupyter:** Navigate to the project's root directory (`Airbnb_data_analysis`) in your terminal and run:

    ```bash
    jupyter lab
    # OR
    # jupyter notebook
    ```

    This will open the Jupyter interface in your web browser.

2.  **Navigate and Run:**
    - In the Jupyter interface, open the `notebooks/` folder.
    - Run the notebooks sequentially (01, 02, 03, 04, 05, 06). **Later notebooks depend on the outputs (processed data, saved models) generated by earlier notebooks.**
      - `01-Data_Understanding.ipynb`: Initial exploration and EDA.
      - `02-Data_Preparation.ipynb`: Cleaning, feature engineering, encoding, saving processed data/scalers.
      - `03-Visualization.ipynb`: Refined visualizations using processed data.
      - `04-Modelling.ipynb`: Baseline models, candidate evaluation, hyperparameter tuning, final model training, test set evaluation, saving the final model.
      - `05-Interpretation.ipynb`: Feature importance and SHAP analysis for the final model.
      - `06-Bonus_Tasks.ipynb`: Analysis for the bonus questions (sentiment, seasonality, investment).

### Running the Streamlit App (Optional Bonus)

The Streamlit app provides an interactive UI to get price predictions from the trained model.

1.  **Prerequisites:**

    - Ensure the virtual environment is activated.
    - **Crucially:** You must have successfully run the `04-Modelling.ipynb` notebook at least up to the point where the final model (`xgb_price_predictor.joblib`), scaler (`standard_scaler.joblib`), feature list (`final_feature_list.joblib`), and potentially feature defaults (`feature_defaults.joblib`) are saved in the `model/` directory. The app relies on these saved artifacts.

2.  **Run the App:** Navigate to the project's root directory in your terminal and run:

    ```bash
    streamlit run streamlit_app/app.py
    ```

3.  **Interact:** The application should open in your web browser, allowing you to input listing details and get a price prediction.

## Key Libraries Used

- Pandas, NumPy: Data manipulation.
- Matplotlib, Seaborn: Data visualization.
- Scikit-learn: Data preprocessing, modeling, evaluation, cross-validation.
- XGBoost, CatBoost, LightGBM: Gradient Boosting models.
- SHAP: Model interpretation.
- Joblib: Saving/loading Python objects (models, scalers).
- Streamlit: Building the interactive web app.
- Category Encoders: For Target Encoding.
- uv / pip: Package installation.
